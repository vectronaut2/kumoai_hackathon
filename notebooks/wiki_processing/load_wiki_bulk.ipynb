{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:39:41.979392Z",
     "start_time": "2025-08-20T19:39:41.976493Z"
    }
   },
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import concurrent.futures\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from wikidata.client import Client"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:39:41.986971Z",
     "start_time": "2025-08-20T19:39:41.984999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "INPUT_FILE = \"fb_wiki_scoped_to_dataset.csv\"\n",
    "OUTPUT_FILE = \"wiki_entities_text.csv\"\n",
    "FAILED_FILE = \"failed_qids.txt\"\n",
    "FAILED_RETRY_FILE = \"failed_qids.txt\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"KrishBot/1.0 (contact: saikdvs@gmail.com)\"\n",
    "}"
   ],
   "id": "e072cb8a5ab8de83",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:39:41.992102Z",
     "start_time": "2025-08-20T19:39:41.990316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunked(iterable, size):\n",
    "    for i in range(0, len(iterable), size):\n",
    "        yield iterable[i:i + size]"
   ],
   "id": "6465d6ee5657dfe0",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:39:41.997999Z",
     "start_time": "2025-08-20T19:39:41.995288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_with_retries(url, params, retries=5, backoff=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
    "            if resp.status_code == 200:\n",
    "                return resp.json()\n",
    "            elif resp.status_code in (429, 503):\n",
    "                wait = backoff * (2 ** attempt)\n",
    "                print(f\"Rate limited ({resp.status_code}), retrying in {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"HTTP error {resp.status_code}: {resp.text[:200]}\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}, retrying...\")\n",
    "            time.sleep(backoff * (2 ** attempt))\n",
    "    return None"
   ],
   "id": "ca50d73ad367cb3a",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:39:42.003579Z",
     "start_time": "2025-08-20T19:39:42.000799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_wikipedia_title_from_qid(qid):\n",
    "    \"\"\"\n",
    "    Fetch the Wikipedia title for a given Wikidata QID.\n",
    "    Returns a tuple: (lang, title), e.g., ('en', 'Python (programming language)')\n",
    "    Only considers Wikipedia sitelinks (ending with 'wiki').\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": qid,\n",
    "        \"props\": \"sitelinks\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    data = fetch_with_retries(\"https://www.wikidata.org/w/api.php\", params)\n",
    "    if not data:\n",
    "        return None, None\n",
    "\n",
    "    entities = data.get(\"entities\", {})\n",
    "    if qid not in entities:\n",
    "        return None, None\n",
    "\n",
    "    sitelinks = entities[qid].get(\"sitelinks\", {})\n",
    "\n",
    "    # Filter only Wikipedia sitelinks ending with 'wiki'\n",
    "    wiki_sitelinks = {k: v for k, v in sitelinks.items() if k.endswith(\"wiki\")}\n",
    "\n",
    "    if not wiki_sitelinks:\n",
    "        return None, None\n",
    "\n",
    "    # Prefer English Wikipedia if available\n",
    "    if \"enwiki\" in wiki_sitelinks:\n",
    "        return \"en\", wiki_sitelinks[\"enwiki\"][\"title\"]\n",
    "\n",
    "    # Otherwise return the first available Wikipedia sitelink\n",
    "    first_key = next(iter(wiki_sitelinks))\n",
    "    lang = first_key.replace(\"wiki\", \"\")\n",
    "    return lang, wiki_sitelinks[first_key][\"title\"]\n"
   ],
   "id": "6e52516fdfbcce29",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:39:42.009331Z",
     "start_time": "2025-08-20T19:39:42.005858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_wikipedia_extracts(triplets):\n",
    "    # Group titles by language\n",
    "    lang_to_titles = defaultdict(list)\n",
    "    for qid,title,lang in triplets:\n",
    "        lang_to_titles[lang].append(title)\n",
    "\n",
    "    for item in lang_to_titles:\n",
    "        print(item)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Fetch all titles for each language in one call\n",
    "    for lang, titles in lang_to_titles.items():\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"extracts\",\n",
    "            \"explaintext\": True,\n",
    "            \"exintro\": True,\n",
    "            \"titles\": \"|\".join(titles),\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        data = fetch_with_retries(f\"https://{lang}.wikipedia.org/w/api.php\", params)\n",
    "        if not data:\n",
    "            continue\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "        for page in pages.values():\n",
    "            if \"title\" in page:\n",
    "                title_norm = page[\"title\"].replace(\"_\", \" \").lower()\n",
    "                results[title_norm] = page.get(\"extract\", \"\")\n",
    "    return results"
   ],
   "id": "86f7de63df0f9aad",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:39:42.018115Z",
     "start_time": "2025-08-20T19:39:42.015882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_batch(qids, writer, lock, failed_qids, row_counter):\n",
    "    results = []\n",
    "    for qid in qids:\n",
    "        lang,title = get_wikipedia_title_from_qid(qid)\n",
    "        if not title:\n",
    "            failed_qids.add(qid)\n",
    "            continue\n",
    "        results.append((qid, title, lang))\n",
    "\n",
    "    if not results:\n",
    "        return row_counter\n",
    "\n",
    "    extracts = get_wikipedia_extracts(results)\n",
    "\n",
    "    with lock:\n",
    "        for qid, title, lang in results:\n",
    "            title_norm = title.replace(\"_\", \" \").lower()\n",
    "            extract = extracts.get(title_norm, \"\")\n",
    "            if not extract:\n",
    "                failed_qids.add(qid)\n",
    "                continue\n",
    "            writer.writerow([qid, title, extract])\n",
    "            row_counter += 1\n",
    "        sys.stdout.flush()\n",
    "    return row_counter"
   ],
   "id": "f5a512e7c2c02ae5",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:39:42.024370Z",
     "start_time": "2025-08-20T19:39:42.021295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch(qid_list, failed_file=FAILED_FILE):\n",
    "    done_qids = set()\n",
    "    row_counter = 0\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        with open(OUTPUT_FILE, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader, None)\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    done_qids.add(row[0])\n",
    "                    row_counter += 1\n",
    "\n",
    "    qid_list = [qid for qid in qid_list if qid not in done_qids]\n",
    "    print(f\"{len(done_qids)} QIDs already processed, {len(qid_list)} remaining.\")\n",
    "\n",
    "    failed_qids = set()\n",
    "\n",
    "    with open(OUTPUT_FILE, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if row_counter == 0:\n",
    "            writer.writerow([\"QID\", \"Title\", \"Extract\"])\n",
    "\n",
    "        from threading import Lock\n",
    "        lock = Lock()\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = []\n",
    "            for batch in chunked(qid_list, 50):\n",
    "                futures.append(executor.submit(process_batch, batch, writer, lock, failed_qids, row_counter))\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                row_counter = future.result()\n",
    "                print(f\"Total written so far: {row_counter}\")\n",
    "\n",
    "    if failed_qids:\n",
    "        with open(failed_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            for qid in failed_qids:\n",
    "                f.write(qid + \"\\n\")"
   ],
   "id": "ebdfed98a2f6c952",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:39:42.421866Z",
     "start_time": "2025-08-20T19:39:42.031496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dedupe_csv(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows from a CSV file and saves the result to a new file.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): The path to the input CSV file.\n",
    "        output_filepath (str): The path to save the deduplicated CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(input_filepath)\n",
    "\n",
    "        # Remove duplicate rows. By default, it keeps the first occurrence.\n",
    "        # Set inplace=True to modify the DataFrame directly.\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        df.sort_values(by=[\"QID\"], inplace=True)\n",
    "\n",
    "        # Save the deduplicated DataFrame to a new CSV file\n",
    "        # index=False prevents pandas from writing the DataFrame index as a column\n",
    "        df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Successfully deduplicated '{input_filepath}' and saved to '{output_filepath}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_filepath}' was not found.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: The file '{input_filepath}' is empty.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "dedupe_csv(input_filepath=OUTPUT_FILE, output_filepath=\"wiki_entities_text_dedup.csv\")"
   ],
   "id": "d286de5e608e1632",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deduplicated 'wiki_entities_text.csv' and saved to 'wiki_entities_text_dedup.csv'.\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:39:42.436824Z",
     "start_time": "2025-08-20T19:39:42.424739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(INPUT_FILE)\n",
    "col = \"wikidata_id\"\n",
    "df[col].astype(str)\n",
    "qids = df[col].to_numpy()"
   ],
   "id": "1c8047c6560ee520",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:39:42.442212Z",
     "start_time": "2025-08-20T19:39:42.440464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retry_failed(batch_size=50, max_workers=5):\n",
    "    if not os.path.exists(FAILED_FILE):\n",
    "        print(\"No failed_qids.txt found, nothing to retry.\")\n",
    "        return\n",
    "\n",
    "    with open(FAILED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        failed_qids = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    if not failed_qids:\n",
    "        print(\"No failed QIDs to retry.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Retrying {len(failed_qids)} failed QIDs in batches of {batch_size}...\")\n",
    "    fetch(failed_qids, failed_file=FAILED_RETRY_FILE)\n"
   ],
   "id": "a925c48bf545e1ff",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:39:42.447398Z",
     "start_time": "2025-08-20T19:39:42.445795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Uncomment to start fetching\")\n",
    "# fetch(qids)\n",
    "# retry_failed(batch_size=50, max_workers=5)"
   ],
   "id": "5534c563674a08bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncomment to start fetching\n"
     ]
    }
   ],
   "execution_count": 97
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
