{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-20T16:09:14.664015Z",
     "start_time": "2025-08-20T16:09:14.530305Z"
    }
   },
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import concurrent.futures\n",
    "import os\n",
    "import sys\n",
    "\n",
    "WIKIDATA_API = \"https://www.wikidata.org/w/api.php\"\n",
    "WIKIPEDIA_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "OUTPUT_FILE = \"../../datasets/fetched/wiki_entities.csv\"\n",
    "FAILED_FILE = \"failed_qids.txt\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"KrishBot/1.0 (contact: saikdvs@gmail.com)\"\n",
    "}\n",
    "\n",
    "def chunked(iterable, size):\n",
    "    for i in range(0, len(iterable), size):\n",
    "        yield iterable[i:i + size]\n",
    "\n",
    "def fetch_with_retries(url, params, retries=5, backoff=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
    "            if resp.status_code == 200:\n",
    "                return resp.json()\n",
    "            elif resp.status_code in (429, 503):\n",
    "                wait = backoff * (2 ** attempt)\n",
    "                print(f\"Rate limited ({resp.status_code}), retrying in {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"HTTP error {resp.status_code}: {resp.text[:200]}\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}, retrying...\")\n",
    "            time.sleep(backoff * (2 ** attempt))\n",
    "    return None\n",
    "\n",
    "def get_wikipedia_title_from_qid(qid):\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": qid,\n",
    "        \"props\": \"sitelinks\",\n",
    "        \"sitefilter\": \"enwiki\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    data = fetch_with_retries(WIKIDATA_API, params)\n",
    "    if not data:\n",
    "        return None\n",
    "    entities = data.get(\"entities\", {})\n",
    "    if qid in entities:\n",
    "        sitelinks = entities[qid].get(\"sitelinks\", {})\n",
    "        if \"enwiki\" in sitelinks:\n",
    "            return sitelinks[\"enwiki\"][\"title\"]\n",
    "    return None\n",
    "\n",
    "def get_wikipedia_extracts(titles):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"exintro\": True,\n",
    "        \"titles\": \"|\".join(titles),\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    data = fetch_with_retries(WIKIPEDIA_API, params)\n",
    "    if not data:\n",
    "        return {}\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    results = {}\n",
    "    for page in pages.values():\n",
    "        if \"title\" in page:\n",
    "            title_norm = page[\"title\"].replace(\"_\", \" \").lower()\n",
    "            results[title_norm] = page.get(\"extract\", \"\")\n",
    "    return results\n",
    "\n",
    "def process_batch(qids, writer, lock, failed_qids, row_counter):\n",
    "    results = []\n",
    "    for qid in qids:\n",
    "        title = get_wikipedia_title_from_qid(qid)\n",
    "        if not title:\n",
    "            failed_qids.add(qid)\n",
    "            continue\n",
    "        results.append((qid, title))\n",
    "\n",
    "    if not results:\n",
    "        return row_counter\n",
    "\n",
    "    titles = [title for _, title in results]\n",
    "    extracts = get_wikipedia_extracts(titles)\n",
    "\n",
    "    with lock:\n",
    "        for qid, title in results:\n",
    "            title_norm = title.replace(\"_\", \" \").lower()\n",
    "            extract = extracts.get(title_norm, \"\")\n",
    "            if not extract:\n",
    "                failed_qids.add(qid)\n",
    "                continue\n",
    "            writer.writerow([qid, title, extract])\n",
    "            row_counter += 1\n",
    "        sys.stdout.flush()\n",
    "    return row_counter\n",
    "\n",
    "def main(qid_list):\n",
    "    done_qids = set()\n",
    "    row_counter = 0\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        with open(OUTPUT_FILE, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader, None)\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    done_qids.add(row[0])\n",
    "                    row_counter += 1\n",
    "\n",
    "    qid_list = [qid for qid in qid_list if qid not in done_qids]\n",
    "    print(f\"{len(done_qids)} QIDs already processed, {len(qid_list)} remaining.\")\n",
    "\n",
    "    failed_qids = set()\n",
    "\n",
    "    with open(OUTPUT_FILE, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if row_counter == 0:\n",
    "            writer.writerow([\"QID\", \"Title\", \"Extract\"])\n",
    "\n",
    "        from threading import Lock\n",
    "        lock = Lock()\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = []\n",
    "            for batch in chunked(qid_list, 50):\n",
    "                futures.append(executor.submit(process_batch, batch, writer, lock, failed_qids, row_counter))\n",
    "                time.sleep(0.1)\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                row_counter = future.result()\n",
    "                print(f\"Total written so far: {row_counter}\")\n",
    "\n",
    "    if failed_qids:\n",
    "        with open(FAILED_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "            for qid in failed_qids:\n",
    "                f.write(qid + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qids = [\"Q128306\", \"Q16562\", \"Q52996\", \"Q128282\"]  # Replace with your 15000 QIDs\n",
    "    main(qids)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 QIDs already processed, 4 remaining.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../datasets/fetched/wiki_entities.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 147\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    146\u001B[39m     qids = [\u001B[33m\"\u001B[39m\u001B[33mQ128306\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mQ16562\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mQ52996\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mQ128282\u001B[39m\u001B[33m\"\u001B[39m]  \u001B[38;5;66;03m# Replace with your 15000 QIDs\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqids\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 122\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m(qid_list)\u001B[39m\n\u001B[32m    118\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(done_qids)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m QIDs already processed, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(qid_list)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m remaining.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    120\u001B[39m failed_qids = \u001B[38;5;28mset\u001B[39m()\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mOUTPUT_FILE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43ma\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mutf-8\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m csvfile:\n\u001B[32m    123\u001B[39m     writer = csv.writer(csvfile)\n\u001B[32m    124\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m row_counter == \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/kumoai_hackathon/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:343\u001B[39m, in \u001B[36m_modified_open\u001B[39m\u001B[34m(file, *args, **kwargs)\u001B[39m\n\u001B[32m    336\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m}:\n\u001B[32m    337\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    338\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIPython won\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m by default \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    339\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    340\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33myou can use builtins\u001B[39m\u001B[33m'\u001B[39m\u001B[33m open.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    341\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m343\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: '../../datasets/fetched/wiki_entities.csv'"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
